{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd176b5-6a92-4eba-bd93-1ade46b635de",
   "metadata": {},
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0e572b-c0bb-4f87-8a70-047964688c18",
   "metadata": {},
   "source": [
    "In the context of Principal Component Analysis (PCA), a projection refers to the process of mapping data from a higher-dimensional space to a lower-dimensional space while preserving the most important information or variability in the data. PCA is a dimensionality reduction technique used in statistics and machine learning to simplify the analysis of high-dimensional data by projecting it onto a lower-dimensional subspace, typically capturing the most significant patterns or features.\n",
    "\n",
    "Here's how projection is used in PCA:\n",
    "\n",
    "1. Centering the Data: Before performing PCA, it's common practice to center the data by subtracting the mean of each feature from the data points. This step ensures that the first principal component (the direction of maximum variance) passes through the origin.\n",
    "\n",
    "2. Covariance Matrix: PCA works by finding the principal components of the data, which are linear combinations of the original features. The first principal component captures the most variance in the data, the second captures the second-most variance, and so on. To find these components, PCA computes the covariance matrix of the centered data.\n",
    "\n",
    "3. Eigenvalue Decomposition: After obtaining the covariance matrix, PCA performs an eigenvalue decomposition (or singular value decomposition) of the matrix. This decomposition yields a set of eigenvectors (principal components) and eigenvalues.\n",
    "\n",
    "4. Selecting Principal Components: The eigenvectors are sorted by their corresponding eigenvalues in descending order. The eigenvector with the largest eigenvalue represents the direction of maximum variance in the data and is the first principal component. The second principal component is the eigenvector with the second-largest eigenvalue, and so on. You can choose to retain a certain number of principal components based on the amount of variance you want to preserve or by setting a desired dimensionality for the lower-dimensional space.\n",
    "\n",
    "5. Projection: To project the data onto the lower-dimensional subspace defined by the selected principal components, you take a dot product between the centered data and the matrix composed of these principal components. The result is a set of new coordinates in the lower-dimensional space. Each data point's projection onto this subspace effectively represents its new representation in terms of the retained principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85556b8e-dd69-4775-bf9e-24b5c67d2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91988429-8f51-4791-a8ae-8010fb9f667e",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) involves solving an optimization problem to find the principal components of a dataset. The optimization problem in PCA can be formulated as follows:\n",
    "\n",
    " Objective: The goal of PCA is to maximize the variance of the data along a set of orthogonal axes (the principal components) in a lower-dimensional subspace.\n",
    "\n",
    "Steps in the Optimization Problem:\n",
    "\n",
    "1. Centering the Data: Before starting the optimization, PCA typically centers the data by subtracting the mean of each feature from the data points. This step ensures that the first principal component passes through the origin, which simplifies the optimization problem.\n",
    "\n",
    "2.Covariance Matrix: PCA aims to find a set of orthogonal axes (principal components) that capture the maximum variance in the data. To do this, it computes the covariance matrix of the centered data. The covariance matrix quantifies how the features in the data are correlated with each other.\n",
    "\n",
    "3. Eigenvalue Decomposition: After obtaining the covariance matrix, PCA performs an eigenvalue decomposition (or singular value decomposition) of the matrix. This decomposition yields a set of eigenvectors and eigenvalues.\n",
    "\n",
    "4. Eigenvectors: These are the directions in which the data varies the most. The eigenvectors are the principal components, and each eigenvector corresponds to a direction in the original feature space.\n",
    "\n",
    "5. Eigenvalues: Eigenvalues indicate the amount of variance in the data that is explained by the corresponding eigenvector (principal component). Larger eigenvalues correspond to principal components that capture more variance in the data.\n",
    "\n",
    "6. Selecting Principal Components: The optimization problem involves choosing a subset of the eigenvectors (principal components) to retain. This selection can be based on the eigenvalues, where you keep the eigenvectors associated with the largest eigenvalues. By doing this, you ensure that you retain the principal components that explain the most variance in the data.\n",
    "\n",
    "7. Projection: Finally, the selected principal components define a new coordinate system for the data. The optimization problem is effectively achieved when you project the centered data onto this lower-dimensional subspace defined by the retained principal components.\n",
    "\n",
    "What PCA Achieves:\n",
    "\n",
    "The optimization problem in PCA aims to achieve several important goals:\n",
    "\n",
    "1. Dimensionality Reduction: By selecting a subset of the principal components, PCA reduces the dimensionality of the data while retaining most of the data's variance. This can be valuable for simplifying data analysis and visualization.\n",
    "\n",
    "2. Data Compression: PCA can be used for data compression by representing the data in a lower-dimensional space, which can save storage and computational resources.\n",
    "\n",
    "3. Feature Extraction: PCA identifies the most important patterns or features in the data, making it a powerful technique for feature selection.\n",
    "\n",
    "4. Noise Reduction: By focusing on the directions of maximum variance, PCA can help reduce the impact of noise in the data.\n",
    "\n",
    "5. Visualization: PCA can be used to project high-dimensional data onto a 2D or 3D space, making it easier to visualize and explore the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8597dae0-482e-4e95-a46e-6dc5219fb18d",
   "metadata": {},
   "source": [
    "Q3.What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37265101-b830-4e87-8881-0787ba0b9ffb",
   "metadata": {},
   "source": [
    "The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental because PCA relies on the covariance matrix of the data to identify the principal components. Here's how covariance matrices are related to PCA:\n",
    "\n",
    "1.Covariance Matrix Calculation: To perform PCA, you typically start by calculating the covariance matrix of your data. \n",
    "\n",
    "2. Variance-Covariance Structure: The covariance matrix encodes information about how the features in your data are related to each other. If two features have a positive covariance, it means they tend to increase or decrease together; if the covariance is negative, they tend to vary in opposite directions. The magnitude of the covariance indicates the strength of the relationship between features. Features with larger covariances contribute more to the overall variance in the data.\n",
    "\n",
    "3. PCA via Eigenvalue Decomposition: PCA aims to find a set of orthogonal axes (principal components) that maximize the variance in the data. These principal components are linear combinations of the original features. The principal components are the eigenvectors of the covariance matrix Î£, and their corresponding eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. Eigenvectors: Each eigenvector corresponds to a principal component, and they are mutually orthogonal (uncorrelated) to each other. The first principal component (the one with the largest eigenvalue) captures the most variance in the data, the second captures the second-most, and so on.\n",
    "\n",
    "5. Eigenvalues: The eigenvalues of the covariance matrix indicate the proportion of total variance explained by each principal component. Larger eigenvalues correspond to principal components that capture more of the overall variance in the data.\n",
    "\n",
    "6. Dimension Reduction: In practice, you often retain a subset of the principal components based on the magnitude of their corresponding eigenvalues. By selecting a subset of the principal components, you can effectively reduce the dimensionality of your data while preserving most of the essential information.\n",
    "\n",
    "7. Projection: The final step in PCA involves projecting your data onto the subspace defined by the retained principal components. This projection is done by taking a dot product between the centered data and the matrix composed of the selected principal components. The result is a lower-dimensional representation of your data that captures the most significant patterns or features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c5edf1-2c70-48b5-a9b0-ca9671262aff",
   "metadata": {},
   "source": [
    "Q4.How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d99acb-2969-48bc-bb8d-ac2ed8fa4c5e",
   "metadata": {},
   "source": [
    "he choice of the number of principal components in PCA has a significant impact on the performance and results of the technique. It affects several aspects of PCA, including data compression, information retention, and computational efficiency. Here's how the choice of the number of principal components impacts PCA:\n",
    "\n",
    "1. Dimensionality Reduction:\n",
    "\n",
    "Choosing More Principal Components: If you retain more principal components, you'll have a higher-dimensional representation of your data in the reduced subspace. This means you retain more of the original data's variance and, theoretically, more information. However, it may not necessarily be more informative as some of the additional components may capture noise or less important variations in the data.\n",
    "\n",
    "Choosing Fewer Principal Components: If you retain fewer principal components, you achieve a more aggressive dimensionality reduction. This simplifies the data representation but may result in the loss of some information. The trade-off is between reducing dimensionality and retaining meaningful patterns in the data.\n",
    "\n",
    "2. Information Retention:\n",
    "\n",
    "Explained Variance: To make an informed choice about the number of principal components to retain, you can examine the cumulative explained variance. This is the fraction of the total variance in the data that is accounted for by the retained principal components. You may set a threshold (e.g., 95% of explained variance) and choose the number of components that surpass it. A higher number of components will be required to meet a higher threshold.\n",
    "\n",
    "Scree Plot: Another common method to decide the number of components is to plot the eigenvalues of the principal components and look for an \"elbow point\" or a point where the eigenvalues start to level off. This indicates diminishing returns in terms of variance explained, and you can select the number of components before this point.\n",
    "\n",
    "3. Computational Efficiency:\n",
    "\n",
    "Reducing Computational Burden: Computing PCA with a lower number of retained components is computationally less intensive than using more components. If you have a large dataset or limited computational resources, choosing a smaller number of components can be advantageous.\n",
    "\n",
    "4. Interpretability and Visualization:\n",
    "\n",
    "Fewer Components for Interpretability: A smaller number of retained components often results in a more interpretable representation of the data. It's easier to understand and visualize relationships between variables when there are fewer dimensions.\n",
    "\n",
    "5. Overfitting and Noise:\n",
    "\n",
    "Regularization: Retaining too many components can lead to overfitting, where the model captures noise in the data rather than meaningful patterns. Choosing a smaller number of components can act as a form of regularization, helping to mitigate overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54eadb4-f4b7-4f4a-98dc-5cf9554efb2e",
   "metadata": {},
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753479b3-b387-432f-8215-1fdb0f99add6",
   "metadata": {},
   "source": [
    "\n",
    "PCA (Principal Component Analysis) can be used as a feature selection technique in the following way:\n",
    "\n",
    "1. Compute Principal Components: Initially, you perform PCA on your dataset, which results in a set of principal components (eigenvectors) ordered by the amount of variance they explain. These principal components are linear combinations of the original features.\n",
    "\n",
    "2. Select Principal Components: Instead of using all the original features, you can select a subset of the principal components to use as your new features. The selection can be based on various criteria:\n",
    "\n",
    "Explained Variance: You can decide to retain a certain percentage of the total variance in the data (e.g., 95% or 99%). To do this, you sum the eigenvalues of the retained principal components and select enough components to reach the desired explained variance.\n",
    "\n",
    "Scree Plot: Another common method is to plot the eigenvalues and identify an \"elbow point\" where the eigenvalues start to level off. The number of components before this point can be chosen.\n",
    "\n",
    "Domain Knowledge: Sometimes, domain knowledge or prior insights may guide you in selecting specific principal components that are known to be more relevant for your task.\n",
    "\n",
    " Benefits of using PCA for feature selection:\n",
    "\n",
    "1. Dimensionality Reduction: PCA inherently reduces the dimensionality of your dataset by selecting a smaller number of principal components. This can be highly beneficial when you're dealing with high-dimensional data, as it simplifies subsequent analysis and reduces computational complexity.\n",
    "\n",
    "2. Noise Reduction: By focusing on the directions of maximum variance, PCA tends to reduce the impact of noisy or less informative features. This can lead to a more robust and interpretable representation of the data.\n",
    "\n",
    "3. Collinearity Handling: If you have highly correlated features in your dataset, PCA can help in reducing multicollinearity by transforming them into a set of uncorrelated principal components.\n",
    "\n",
    "Interpretability: The resulting principal components are often more interpretable than the original features, especially when you're dealing with a large number of features. They represent linear combinations of the original features, making it easier to understand their relationships.\n",
    "\n",
    "Improved Model Performance: In some cases, using a smaller number of principal components may lead to improved model performance because it focuses on the most informative aspects of the data and reduces the risk of overfitting.\n",
    "\n",
    "Visualization: When dealing with 3D or 2D visualizations, PCA can help project high-dimensional data into a lower-dimensional space for better visualization and exploration.\n",
    "\n",
    "Data Compression: If storage space or memory is a concern, PCA can be used for data compression by representing the data using a smaller number of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fc7548-8310-4ce8-a4cf-f8723732023d",
   "metadata": {},
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a3592-d30a-4d33-9c70-b3426a2faf6b",
   "metadata": {},
   "source": [
    "Applications of PCA in Machine Learning:- \n",
    "\n",
    "1. PCA is used to visualize multidimensional data.\n",
    "\n",
    "2. It is used to reduce the number of dimensions in healthcare data.\n",
    "\n",
    "3. PCA can help resize an image.\n",
    "\n",
    "4. It can be used in finance to analyze stock data and forecast returns.\n",
    "\n",
    "5. PCA helps to find patterns in the high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66634ecb-65c3-42a1-a112-a4dd8dd24df6",
   "metadata": {},
   "source": [
    "Q7. What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e31714-8aa0-4ed2-b056-bd5b36ab6544",
   "metadata": {},
   "source": [
    " Principal Component Analysis (PCA), the terms \"spread\" and \"variance\" are often used interchangeably and are closely related concepts. Both spread and variance describe how data points are distributed along the principal components, but they may be used in slightly different contexts or with different emphases:\n",
    "\n",
    "Variance: Variance quantifies the spread or dispersion of data points along a single dimension or variable. In PCA, it represents the spread of the data points along each principal component. Higher variance along a principal component indicates that the data points are more spread out along that direction in the feature space.\n",
    "\n",
    "Spread: Spread, in the context of PCA, is a more general term that can refer to the overall distribution or arrangement of data points in the feature space, taking into account all the principal components. It encompasses the idea of variance but also considers the covariance between different dimensions. A dataset with a wide spread generally means that data points are distributed over a larger region of the feature space.n the context of Principal Component Analysis (PCA), the terms \"spread\" and \"variance\" are often used interchangeably and are closely related concepts. Both spread and variance describe how data points are distributed along the principal components, but they may be used in slightly different contexts or with different emphases:\n",
    "\n",
    "Variance: Variance quantifies the spread or dispersion of data points along a single dimension or variable. In PCA, it represents the spread of the data points along each principal component. Higher variance along a principal component indicates that the data points are more spread out along that direction in the feature space.\n",
    "\n",
    "Spread: Spread, in the context of PCA, is a more general term that can refer to the overall distribution or arrangement of data points in the feature space, taking into account all the principal components. It encompasses the idea of variance but also considers the covariance between different dimensions. A dataset with a wide spread generally means that data points are distributed over a larger region of the feature space.The variance explained can be understood as the ratio of the vertical spread of the regression line (i.e., from the lowest point on the line to the highest point on the line) to the vertical spread of the data (i.e., from the lowest data point to the highest data point).\n",
    "\n",
    "HOW DO YOU DO A PRINCIPAL COMPONENT ANALYSIS?\n",
    "\n",
    "1. Standardize the range of continuous initial variables\n",
    "2. Compute the covariance matrix to identify correlations\n",
    "3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "4. Create a feature vector to decide which principal components to keep\n",
    "5. Recast the data along the principal components axes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705dd32d-5a2b-4cff-bb3c-b0388936bd0a",
   "metadata": {},
   "source": [
    "Q8.How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17933d79-f0b9-405d-81e8-38ac94b4a31a",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that uses the spread and variance of the data to identify the principal components. The principal components are linear combinations of the original features that capture the maximum variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64537d3-1060-4714-806c-e23cc5e6bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      "[[ 0.63106527 -0.77572974]\n",
      " [-0.77572974 -0.63106527]]\n",
      "\n",
      "Explained Variance Ratios:\n",
      "[0.53490112 0.46509888]\n",
      "\n",
      "Transformed Data:\n",
      "[[-1.55097679e-01 -1.71091132e-01]\n",
      " [ 1.10597833e-02 -1.05467336e-01]\n",
      " [-1.80326601e-01 -3.02719752e-02]\n",
      " [-3.62269905e-01 -1.96245387e-01]\n",
      " [ 3.64045949e-01 -2.83547495e-01]\n",
      " [ 1.42709496e-01 -2.41960879e-01]\n",
      " [-3.06180805e-01 -3.18789956e-01]\n",
      " [ 3.05984435e-02  5.95881935e-01]\n",
      " [-5.79770005e-01  1.64849516e-01]\n",
      " [-1.30467758e-01 -4.46702790e-01]\n",
      " [ 5.09998218e-02 -5.57493572e-01]\n",
      " [-2.60897256e-01 -1.44577125e-01]\n",
      " [-3.68408042e-01  2.10390073e-01]\n",
      " [-5.88983650e-01 -1.38015701e-03]\n",
      " [ 6.10135942e-02  3.94789818e-02]\n",
      " [-3.80285398e-01  1.21553472e-02]\n",
      " [-9.97316456e-02 -6.59730739e-03]\n",
      " [-4.13901790e-01  3.01626876e-01]\n",
      " [-3.89428560e-02 -1.58175480e-01]\n",
      " [ 1.20017192e-01 -4.56395568e-01]\n",
      " [-5.87868918e-02  1.51294336e-01]\n",
      " [ 4.46890970e-01  1.26791522e-01]\n",
      " [-4.61015828e-02 -2.34476044e-01]\n",
      " [ 8.61120070e-02  4.61410075e-01]\n",
      " [-2.97265424e-02  2.31758605e-01]\n",
      " [ 7.29539816e-02 -1.31337810e-02]\n",
      " [ 5.97928048e-01 -1.25136924e-01]\n",
      " [ 6.00411187e-02  4.42142247e-01]\n",
      " [ 2.69026993e-01  3.94919080e-02]\n",
      " [ 1.58023174e-01  1.89991356e-01]\n",
      " [ 6.80577465e-02  5.12999641e-01]\n",
      " [ 3.60353025e-01  1.09634149e-01]\n",
      " [-1.08615937e-01  3.20786560e-01]\n",
      " [ 4.96134807e-01  7.82488469e-03]\n",
      " [ 5.07610376e-01 -4.69216072e-03]\n",
      " [ 3.06021820e-01 -3.47247150e-01]\n",
      " [ 2.00562181e-01 -4.33428632e-01]\n",
      " [ 4.89483275e-01  1.07772193e-01]\n",
      " [ 1.38588447e-01  4.10737346e-01]\n",
      " [ 1.48142013e-01  4.01321294e-01]\n",
      " [-6.73291422e-02  1.97874996e-01]\n",
      " [-4.43331117e-01  2.19214770e-01]\n",
      " [ 2.05050820e-01  9.89633037e-02]\n",
      " [ 3.10690067e-01  2.40789322e-01]\n",
      " [-3.04064027e-01 -3.27254385e-01]\n",
      " [-2.63333437e-01  3.76680729e-02]\n",
      " [-4.19144420e-01  1.51682254e-01]\n",
      " [ 9.38859888e-02  3.65864373e-01]\n",
      " [ 4.07888763e-01  2.38306292e-01]\n",
      " [ 5.72831683e-01  5.99744065e-02]\n",
      " [ 2.71652101e-01  9.77589277e-03]\n",
      " [-2.29084016e-01 -4.71544645e-01]\n",
      " [-2.36604067e-01  1.49412900e-01]\n",
      " [-1.69368803e-02 -1.14421842e-01]\n",
      " [-5.44937830e-01 -6.83268725e-02]\n",
      " [-3.21060241e-01 -1.75016575e-01]\n",
      " [ 2.64045229e-01 -2.43380128e-02]\n",
      " [ 2.59337085e-01 -1.75537180e-01]\n",
      " [ 1.58481812e-01 -3.44348082e-01]\n",
      " [ 7.25740531e-02 -4.15049983e-01]\n",
      " [ 1.22149395e-01 -1.72998726e-01]\n",
      " [ 1.57147666e-01 -4.42091364e-01]\n",
      " [-1.49558208e-01 -5.49966918e-03]\n",
      " [-1.68469550e-01  5.00768866e-01]\n",
      " [ 2.44949605e-01  1.07968431e-02]\n",
      " [ 1.10758281e-01 -4.40229902e-02]\n",
      " [-9.25346514e-02  4.12644116e-01]\n",
      " [-4.53136725e-02 -1.09047016e-01]\n",
      " [-9.09117433e-02 -1.51762533e-01]\n",
      " [ 1.30214456e-01 -7.21380977e-02]\n",
      " [ 3.34009585e-01 -2.21462395e-01]\n",
      " [-3.63473415e-01 -1.95004242e-01]\n",
      " [ 1.60925590e-02 -3.63617298e-01]\n",
      " [-5.96661459e-01  4.79684753e-02]\n",
      " [-2.70743610e-01 -4.78424877e-01]\n",
      " [-5.25760724e-01  4.21952969e-02]\n",
      " [-3.21605371e-01  1.91462130e-01]\n",
      " [-5.26327870e-01  7.47716108e-02]\n",
      " [ 1.21361428e-01 -2.79430043e-01]\n",
      " [ 2.56663184e-01  3.46457910e-01]\n",
      " [ 1.41655370e-01 -1.21260282e-01]\n",
      " [-1.63055485e-01 -4.00892803e-01]\n",
      " [ 5.10447629e-03 -5.90837815e-01]\n",
      " [-2.18494495e-01  4.69714376e-01]\n",
      " [ 3.80892287e-01  3.13860748e-02]\n",
      " [ 3.40015354e-01  2.67496584e-01]\n",
      " [ 1.65201797e-01  5.39139279e-01]\n",
      " [ 3.80528855e-01 -5.10350140e-02]\n",
      " [-4.48641967e-01 -1.47608445e-01]\n",
      " [ 4.73191901e-01  1.39443349e-01]\n",
      " [-3.24807674e-01  1.86019405e-01]\n",
      " [ 2.33087220e-01  1.08071713e-01]\n",
      " [ 1.66637196e-01 -4.06179179e-01]\n",
      " [-6.62326123e-02 -8.18069989e-02]\n",
      " [ 2.72128486e-01 -5.72640316e-02]\n",
      " [ 1.41880287e-01  2.64633970e-01]\n",
      " [-5.61718967e-01 -3.44250728e-02]\n",
      " [ 1.39600178e-01 -1.77232072e-01]\n",
      " [-4.39571430e-04  3.69043161e-01]\n",
      " [-2.47010865e-01  3.86810804e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 2)\n",
    "\n",
    "# Create a PCA instance and fit it to the data\n",
    "pca = PCA(n_components=2)  \n",
    "pca.fit(X)\n",
    "\n",
    "components = pca.components_\n",
    "explained_variance_ratios = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Principal Components:\")\n",
    "print(components)\n",
    "print(\"\\nExplained Variance Ratios:\")\n",
    "print(explained_variance_ratios)\n",
    "\n",
    "# Transform the data into the new feature space\n",
    "X_transformed = pca.transform(X)\n",
    "\n",
    "# The transformed data now has reduced dimensions\n",
    "print(\"\\nTransformed Data:\")\n",
    "print(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366979f9-228f-4c68-8f45-89eb6031fa3d",
   "metadata": {},
   "source": [
    "Q9.  How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f29ec-3c76-4a05-9bd4-839cfa6b0e51",
   "metadata": {},
   "source": [
    "PCA is particularly useful for handling data with high variance in some dimensions and low variance in others because it identifies the principal components that capture the maximum variance in the data. This allows you to reduce the dimensionality of the data while retaining the most important information.\n",
    "\n",
    "Here's how you can use PCA to handle data with varying variances in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc91006-43f1-4113-8e5d-04921f8f0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal Components:\n",
      "[[-0.99997547  0.00700418]\n",
      " [ 0.00700418  0.99997547]]\n",
      "\n",
      "Explained Variance Ratios:\n",
      "[0.98986463 0.01013537]\n",
      "\n",
      "Transformed Data:\n",
      "[[-0.03468297  0.02307185]\n",
      " [-0.08875081  0.00641952]\n",
      " [ 0.09042413  0.01526586]\n",
      " [ 0.07666427  0.03995073]\n",
      " [-0.44975442 -0.00719645]\n",
      " [-0.27771903  0.00614425]\n",
      " [-0.05376619  0.04424676]\n",
      " [ 0.44264285 -0.04307906]\n",
      " [ 0.49398142  0.03111223]\n",
      " [-0.26391215  0.04016011]\n",
      " [-0.46441846  0.03447899]\n",
      " [ 0.05269479  0.02899396]\n",
      " [ 0.39579282  0.01252963]\n",
      " [ 0.37092803  0.04317932]\n",
      " [-0.00792895 -0.00716903]\n",
      " [ 0.2496093   0.02698514]\n",
      " [ 0.05787514  0.00774764]\n",
      " [ 0.4952594   0.00960432]\n",
      " [-0.09803246  0.01368979]\n",
      " [-0.42963123  0.02250121]\n",
      " [ 0.15442316 -0.00606914]\n",
      " [-0.18395576 -0.04138059]\n",
      " [-0.1526645   0.01944297]\n",
      " [ 0.30332904 -0.03792345]\n",
      " [ 0.19845027 -0.01370982]\n",
      " [-0.05625944 -0.00443649]\n",
      " [-0.47466198 -0.03516235]\n",
      " [ 0.30485749 -0.03469576]\n",
      " [-0.13929876 -0.0223863 ]\n",
      " [ 0.047488   -0.02458124]\n",
      " [ 0.35472776 -0.04013863]\n",
      " [-0.14260057 -0.03387432]\n",
      " [ 0.31729686 -0.01404081]\n",
      " [-0.30728894 -0.03682905]\n",
      " [-0.32424091 -0.0368106 ]\n",
      " [-0.46249112  0.00141394]\n",
      " [-0.46269735  0.01503515]\n",
      " [-0.22560186 -0.04319271]\n",
      " [ 0.2309003  -0.03828913]\n",
      " [ 0.21756636 -0.03834262]\n",
      " [ 0.19593091 -0.00863683]\n",
      " [ 0.44995523  0.01740549]\n",
      " [-0.05278554 -0.02178245]\n",
      " [-0.00955328 -0.03923058]\n",
      " [-0.06166534  0.04467205]\n",
      " [ 0.19552247  0.01668139]\n",
      " [ 0.38232324  0.02026478]\n",
      " [ 0.22434545 -0.03194358]\n",
      " [-0.07286833 -0.04617058]\n",
      " [-0.31530027 -0.04601374]\n",
      " [-0.16399465 -0.02054164]\n",
      " [-0.22088592  0.0490766 ]\n",
      " [ 0.26527265  0.00706731]\n",
      " [-0.07801046  0.00908123]\n",
      " [ 0.2912073   0.04454575]\n",
      " [ 0.06709457  0.03548121]\n",
      " [-0.18563765 -0.01764708]\n",
      " [-0.299884   -0.00693974]\n",
      " [-0.36705831  0.01200794]\n",
      " [-0.36761253  0.02313797]\n",
      " [-0.21126922  0.0029217 ]\n",
      " [-0.44199298  0.0188047 ]\n",
      " [ 0.09019621  0.01131727]\n",
      " [ 0.49463464 -0.02199815]\n",
      " [-0.14633803 -0.0186583 ]\n",
      " [-0.10408381 -0.00508481]\n",
      " [ 0.37835432 -0.02151294]\n",
      " [-0.05592093  0.01078864]\n",
      " [-0.06023751  0.01705183]\n",
      " [-0.13816896 -0.00458109]\n",
      " [-0.38265102 -0.00925447]\n",
      " [ 0.07838662  0.0399537 ]\n",
      " [-0.29206506  0.02374453]\n",
      " [ 0.41403573  0.04035869]\n",
      " [-0.19990803  0.0525956 ]\n",
      " [ 0.36477955  0.03556792]\n",
      " [ 0.35155834  0.01040325]\n",
      " [ 0.39039315  0.03337668]\n",
      " [-0.29328441  0.01027396]\n",
      " [ 0.10649127 -0.04252081]\n",
      " [-0.18347786 -0.00205125]\n",
      " [-0.20781492  0.03940419]\n",
      " [-0.46128202  0.04012164]\n",
      " [ 0.50215448 -0.01621035]\n",
      " [-0.21623631 -0.03001379]\n",
      " [-0.00736963 -0.04320622]\n",
      " [ 0.31363749 -0.04903638]\n",
      " [-0.27990526 -0.0243382 ]\n",
      " [ 0.16892297  0.04293543]\n",
      " [-0.19075869 -0.04417166]\n",
      " [ 0.34936132  0.01101054]\n",
      " [-0.06343167 -0.02445761]\n",
      " [-0.42014491  0.01564917]\n",
      " [-0.02159034  0.0104519 ]\n",
      " [-0.21626949 -0.01598168]\n",
      " [ 0.11555182 -0.02851625]\n",
      " [ 0.32808915  0.04344973]\n",
      " [-0.22557299  0.0019353 ]\n",
      " [ 0.28638524 -0.02526145]\n",
      " [ 0.45589267 -0.00844228]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(0)\n",
    "high_variance_data = np.random.rand(100, 2)\n",
    "high_variance_data[:, 1] *= 0.1  # Reduce the variance in the second dimension\n",
    "\n",
    "# Create a PCA instance and fit it to the data\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(high_variance_data)\n",
    "\n",
    "components = pca.components_\n",
    "explained_variance_ratios = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Principal Components:\")\n",
    "print(components)\n",
    "print(\"\\nExplained Variance Ratios:\")\n",
    "print(explained_variance_ratios)\n",
    "\n",
    "data_transformed = pca.transform(high_variance_data)\n",
    "\n",
    "print(\"\\nTransformed Data:\")\n",
    "print(data_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e0383-b590-4dad-941b-f8ad7e7f843c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
